{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "635ff405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fdbd800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  QueryId                                              Query Label      Lang  \\\n",
      "0   50313                             ganne ki varity .....?     7  hin_Latn   \n",
      "1   88853                     Panjikaran galat ho gaya hai ?     3  hin_Latn   \n",
      "2   88845  36 din moonfali bubai ki hui hai kherpatwar la...     9  hin_Latn   \n",
      "3   88843           AMROOD KE FAL ME KEET LAGA HUA HAI ....?     6  hin_Latn   \n",
      "4   12623                       aam men baur gir raha hai .?     6  hin_Latn   \n",
      "\n",
      "   Confidence  \n",
      "0    0.995308  \n",
      "1    0.997592  \n",
      "2    0.999946  \n",
      "3    0.994257  \n",
      "4    0.999745  \n"
     ]
    }
   ],
   "source": [
    "# Replace 'your_file.csv' with the path to your actual CSV file\n",
    "df = pd.read_csv('sorted_file.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e400f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "6     37883\n",
      "8     23099\n",
      "5      8659\n",
      "0      8579\n",
      "1      5049\n",
      "3      4783\n",
      "4      4082\n",
      "9      3037\n",
      "2      2759\n",
      "7      2669\n",
      "2\"        1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each unique value in column 'i'\n",
    "value_counts = df['Label'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b18d6a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "6    37883\n",
      "8    23099\n",
      "5     8659\n",
      "0     8579\n",
      "1     5049\n",
      "3     4783\n",
      "4     4082\n",
      "9     3037\n",
      "2     2759\n",
      "7     2669\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replace 'row_label' with the label (index) of the row you want to remove\n",
    "dm = df[df['Label'] != '2\"']\n",
    "\n",
    "\n",
    "# Count the occurrences of each unique value in column 'i'\n",
    "value_counts = dm['Label'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "482dc07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "6    37883\n",
      "8    23099\n",
      "5     8659\n",
      "0     8579\n",
      "1     5049\n",
      "3     4783\n",
      "4     4082\n",
      "9     3037\n",
      "2     2759\n",
      "7     2669\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replace 'row_label' with the label (index) of the row you want to remove\n",
    "dm = df[df['Label'] != '2\"']\n",
    "\n",
    "\n",
    "# Count the occurrences of each unique value in column 'i'\n",
    "value_counts = dm['Label'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee9e4fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before under-sampling: Counter({6.0: 28360, 8.0: 17308, 5.0: 6476, 0.0: 6449, 1.0: 3812, 3.0: 3561, 4.0: 3088, 9.0: 2276, 2.0: 2115, 7.0: 2004})\n",
      "Class distribution after under-sampling: Counter({np.float64(0.0): 2004, np.float64(1.0): 2004, np.float64(2.0): 2004, np.float64(3.0): 2004, np.float64(4.0): 2004, np.float64(5.0): 2004, np.float64(6.0): 2004, np.float64(7.0): 2004, np.float64(8.0): 2004, np.float64(9.0): 2004})\n",
      "Accuracy: 65.14%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.30      0.38      0.33      2130\n",
      "         1.0       0.46      0.59      0.52      1237\n",
      "         2.0       0.17      0.32      0.22       644\n",
      "         3.0       0.50      0.68      0.58      1222\n",
      "         4.0       0.70      0.74      0.72       994\n",
      "         5.0       0.51      0.54      0.52      2183\n",
      "         6.0       0.87      0.67      0.76      9523\n",
      "         7.0       0.48      0.74      0.58       665\n",
      "         8.0       0.89      0.75      0.82      5791\n",
      "         9.0       0.50      0.82      0.62       761\n",
      "\n",
      "    accuracy                           0.65     25150\n",
      "   macro avg       0.54      0.62      0.57     25150\n",
      "weighted avg       0.71      0.65      0.67     25150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# Sample text data\n",
    "# Assuming 'df' is your DataFrame containing 'Query' and 'Label' columns.\n",
    "\n",
    "# Ensure that the 'Label' column is of integer type and handle any invalid entries (e.g., strings or NaN)\n",
    "df['Label'] = pd.to_numeric(df['Label'], errors='coerce')  # Convert to numeric, NaN if invalid\n",
    "\n",
    "# Remove any rows where 'Label' is NaN after conversion\n",
    "df = df.dropna(subset=['Label'])\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Query'], df['Label'], test_size=0.25, random_state=42)\n",
    "\n",
    "# Check the class distribution before under-sampling\n",
    "print(\"Class distribution before under-sampling:\", Counter(y_train))\n",
    "\n",
    "# Initialize the random under-sampler\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Apply random under-sampling on the training set (only on labels, not on the text)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(np.array(X_train).reshape(-1, 1), np.array(y_train))\n",
    "\n",
    "# Check the class distribution after under-sampling\n",
    "print(\"Class distribution after under-sampling:\", Counter(y_train_resampled))\n",
    "\n",
    "# Create a Naive Bayes pipeline with TF-IDF and MultinomialNB with alpha=0.1\n",
    "# Use n-grams from 2 to 5\n",
    "model = make_pipeline(TfidfVectorizer(max_features =15000,ngram_range=(2, 5)),  MultinomialNB(alpha=0.1))\n",
    "\n",
    "# Train the model on the resampled data\n",
    "model.fit(X_train_resampled.flatten(), y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d01b472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before under-sampling: Counter({6.0: 28360, 8.0: 17308, 5.0: 6476, 0.0: 6449, 1.0: 3812, 3.0: 3561, 4.0: 3088, 9.0: 2276, 2.0: 2115, 7.0: 2004})\n",
      "Class distribution after under-sampling: Counter({np.float64(0.0): 2004, np.float64(1.0): 2004, np.float64(2.0): 2004, np.float64(3.0): 2004, np.float64(4.0): 2004, np.float64(5.0): 2004, np.float64(6.0): 2004, np.float64(7.0): 2004, np.float64(8.0): 2004, np.float64(9.0): 2004})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [00:41:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.58%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.29      0.34      0.32      2130\n",
      "         1.0       0.46      0.56      0.50      1237\n",
      "         2.0       0.07      0.53      0.12       644\n",
      "         3.0       0.37      0.57      0.45      1222\n",
      "         4.0       0.76      0.70      0.73       994\n",
      "         5.0       0.50      0.46      0.48      2183\n",
      "         6.0       0.88      0.51      0.65      9523\n",
      "         7.0       0.39      0.66      0.49       665\n",
      "         8.0       0.91      0.59      0.71      5791\n",
      "         9.0       0.57      0.78      0.66       761\n",
      "\n",
      "    accuracy                           0.54     25150\n",
      "   macro avg       0.52      0.57      0.51     25150\n",
      "weighted avg       0.71      0.54      0.59     25150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# Sample text data\n",
    "# Assuming 'df' is your DataFrame containing 'Query' and 'Label' columns.\n",
    "\n",
    "# Ensure that the 'Label' column is of integer type and handle any invalid entries (e.g., strings or NaN)\n",
    "df['Label'] = pd.to_numeric(df['Label'], errors='coerce')  # Convert to numeric, NaN if invalid\n",
    "\n",
    "# Remove any rows where 'Label' is NaN after conversion\n",
    "df = df.dropna(subset=['Label'])\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Query'], df['Label'], test_size=0.25, random_state=42)\n",
    "\n",
    "# Check the class distribution before under-sampling\n",
    "print(\"Class distribution before under-sampling:\", Counter(y_train))\n",
    "\n",
    "# Initialize the random under-sampler\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Apply random under-sampling on the training set (only on labels, not on the text)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(np.array(X_train).reshape(-1, 1), np.array(y_train))\n",
    "\n",
    "# Check the class distribution after under-sampling\n",
    "print(\"Class distribution after under-sampling:\", Counter(y_train_resampled))\n",
    "\n",
    "# Create an XGBoost pipeline with TF-IDF and XGBClassifier with default parameters\n",
    "model = make_pipeline(TfidfVectorizer(ngram_range=(2, 5)), XGBClassifier(eval_metric='mlogloss', use_label_encoder=False))\n",
    "\n",
    "# Train the model on the resampled data\n",
    "model.fit(X_train_resampled.flatten(), y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc48670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before under-sampling: Counter({6.0: 28360, 8.0: 17308, 5.0: 6476, 0.0: 6449, 1.0: 3812, 3.0: 3561, 4.0: 3088, 9.0: 2276, 2.0: 2115, 7.0: 2004})\n",
      "Class distribution after under-sampling: Counter({np.float64(0.0): 2004, np.float64(1.0): 2004, np.float64(2.0): 2004, np.float64(3.0): 2004, np.float64(4.0): 2004, np.float64(5.0): 2004, np.float64(6.0): 2004, np.float64(7.0): 2004, np.float64(8.0): 2004, np.float64(9.0): 2004})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.20%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.37      0.38      2130\n",
      "         1.0       0.47      0.60      0.53      1237\n",
      "         2.0       0.15      0.37      0.21       644\n",
      "         3.0       0.37      0.76      0.50      1222\n",
      "         4.0       0.75      0.75      0.75       994\n",
      "         5.0       0.47      0.58      0.52      2183\n",
      "         6.0       0.88      0.67      0.76      9523\n",
      "         7.0       0.48      0.74      0.58       665\n",
      "         8.0       0.91      0.73      0.81      5791\n",
      "         9.0       0.64      0.81      0.71       761\n",
      "\n",
      "    accuracy                           0.65     25150\n",
      "   macro avg       0.55      0.64      0.58     25150\n",
      "weighted avg       0.72      0.65      0.67     25150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# Clean and prepare your data\n",
    "df['Label'] = pd.to_numeric(df['Label'], errors='coerce')\n",
    "df = df.dropna(subset=['Label'])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Query'], df['Label'], test_size=0.25, random_state=42)\n",
    "\n",
    "# Check original class distribution\n",
    "print(\"Class distribution before under-sampling:\", Counter(y_train))\n",
    "\n",
    "# Under-sample\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(np.array(X_train).reshape(-1, 1), np.array(y_train))\n",
    "\n",
    "# Class distribution after under-sampling\n",
    "print(\"Class distribution after under-sampling:\", Counter(y_train_resampled))\n",
    "\n",
    "# Create pipeline with Logistic Regression\n",
    "model = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(2, 5)),\n",
    "    LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    ")\n",
    "\n",
    "# Train\n",
    "model.fit(X_train_resampled.flatten(), y_train_resampled)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2af0ab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before under-sampling: Counter({6.0: 28360, 8.0: 17308, 5.0: 6476, 0.0: 6449, 1.0: 3812, 3.0: 3561, 4.0: 3088, 9.0: 2276, 2.0: 2115, 7.0: 2004})\n",
      "Class distribution after under-sampling: Counter({np.float64(0.0): 2004, np.float64(1.0): 2004, np.float64(2.0): 2004, np.float64(3.0): 2004, np.float64(4.0): 2004, np.float64(5.0): 2004, np.float64(6.0): 2004, np.float64(7.0): 2004, np.float64(8.0): 2004, np.float64(9.0): 2004})\n",
      "SVM Accuracy: 62.81%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.37      0.38      2130\n",
      "         1.0       0.39      0.59      0.47      1237\n",
      "         2.0       0.14      0.39      0.21       644\n",
      "         3.0       0.41      0.75      0.53      1222\n",
      "         4.0       0.71      0.77      0.74       994\n",
      "         5.0       0.48      0.55      0.52      2183\n",
      "         6.0       0.88      0.64      0.74      9523\n",
      "         7.0       0.39      0.73      0.51       665\n",
      "         8.0       0.90      0.68      0.78      5791\n",
      "         9.0       0.57      0.82      0.67       761\n",
      "\n",
      "    accuracy                           0.63     25150\n",
      "   macro avg       0.53      0.63      0.55     25150\n",
      "weighted avg       0.71      0.63      0.65     25150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#svm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# Clean and prepare your data\n",
    "df['Label'] = pd.to_numeric(df['Label'], errors='coerce')\n",
    "df = df.dropna(subset=['Label'])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Query'], df['Label'], test_size=0.25, random_state=42)\n",
    "\n",
    "# Check original class distribution\n",
    "print(\"Class distribution before under-sampling:\", Counter(y_train))\n",
    "\n",
    "# Under-sample\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(np.array(X_train).reshape(-1, 1), np.array(y_train))\n",
    "\n",
    "# Class distribution after under-sampling\n",
    "print(\"Class distribution after under-sampling:\", Counter(y_train_resampled))\n",
    "\n",
    "# Create pipeline with SVM (LinearSVC)\n",
    "model = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(2, 5)),\n",
    "    LinearSVC()\n",
    ")\n",
    "\n",
    "# Train\n",
    "model.fit(X_train_resampled.flatten(), y_train_resampled)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"SVM Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4965150a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before under-sampling: Counter({6.0: 28360, 8.0: 17308, 5.0: 6476, 0.0: 6449, 1.0: 3812, 3.0: 3561, 4.0: 3088, 9.0: 2276, 2.0: 2115, 7.0: 2004})\n",
      "Class distribution after under-sampling: Counter({np.float64(0.0): 2004, np.float64(1.0): 2004, np.float64(2.0): 2004, np.float64(3.0): 2004, np.float64(4.0): 2004, np.float64(5.0): 2004, np.float64(6.0): 2004, np.float64(7.0): 2004, np.float64(8.0): 2004, np.float64(9.0): 2004})\n",
      "\n",
      "Random Forest Accuracy: 54.98%\n",
      "Classification Report (Random Forest):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.32      0.34      0.33      2130\n",
      "         1.0       0.39      0.58      0.47      1237\n",
      "         2.0       0.09      0.50      0.15       644\n",
      "         3.0       0.40      0.61      0.48      1222\n",
      "         4.0       0.70      0.71      0.71       994\n",
      "         5.0       0.48      0.48      0.48      2183\n",
      "         6.0       0.88      0.53      0.66      9523\n",
      "         7.0       0.32      0.69      0.44       665\n",
      "         8.0       0.89      0.60      0.72      5791\n",
      "         9.0       0.48      0.78      0.59       761\n",
      "\n",
      "    accuracy                           0.55     25150\n",
      "   macro avg       0.50      0.58      0.50     25150\n",
      "weighted avg       0.70      0.55      0.59     25150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# Clean and prepare your data\n",
    "df['Label'] = pd.to_numeric(df['Label'], errors='coerce')\n",
    "df = df.dropna(subset=['Label'])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Query'], df['Label'], test_size=0.25, random_state=42)\n",
    "\n",
    "# Check original class distribution\n",
    "print(\"Class distribution before under-sampling:\", Counter(y_train))\n",
    "\n",
    "# Under-sample\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(np.array(X_train).reshape(-1, 1), np.array(y_train))\n",
    "\n",
    "# Class distribution after under-sampling\n",
    "print(\"Class distribution after under-sampling:\", Counter(y_train_resampled))\n",
    "\n",
    "# Create pipeline with Random Forest\n",
    "model_rf = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(2, 5)),\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42)\n",
    ")\n",
    "\n",
    "# Train\n",
    "model_rf.fit(X_train_resampled.flatten(), y_train_resampled)\n",
    "\n",
    "# Predict\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"\\nRandom Forest Accuracy: {accuracy_rf * 100:.2f}%\")\n",
    "print(\"Classification Report (Random Forest):\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b3d39f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before under-sampling: Counter({6.0: 28360, 8.0: 17308, 5.0: 6476, 0.0: 6449, 1.0: 3812, 3.0: 3561, 4.0: 3088, 9.0: 2276, 2.0: 2115, 7.0: 2004})\n",
      "Class distribution after under-sampling: Counter({np.float64(0.0): 2004, np.float64(1.0): 2004, np.float64(2.0): 2004, np.float64(3.0): 2004, np.float64(4.0): 2004, np.float64(5.0): 2004, np.float64(6.0): 2004, np.float64(7.0): 2004, np.float64(8.0): 2004, np.float64(9.0): 2004})\n",
      "\n",
      "SGD Classifier Accuracy: 64.87%\n",
      "Classification Report (SGD Classifier):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.35      0.36      2130\n",
      "         1.0       0.49      0.58      0.53      1237\n",
      "         2.0       0.15      0.31      0.21       644\n",
      "         3.0       0.34      0.77      0.47      1222\n",
      "         4.0       0.77      0.73      0.75       994\n",
      "         5.0       0.47      0.57      0.51      2183\n",
      "         6.0       0.87      0.67      0.75      9523\n",
      "         7.0       0.48      0.72      0.58       665\n",
      "         8.0       0.90      0.75      0.82      5791\n",
      "         9.0       0.64      0.80      0.71       761\n",
      "\n",
      "    accuracy                           0.65     25150\n",
      "   macro avg       0.55      0.62      0.57     25150\n",
      "weighted avg       0.72      0.65      0.67     25150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sgd classifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# Clean and prepare your data\n",
    "df['Label'] = pd.to_numeric(df['Label'], errors='coerce')\n",
    "df = df.dropna(subset=['Label'])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Query'], df['Label'], test_size=0.25, random_state=42)\n",
    "\n",
    "# Check original class distribution\n",
    "print(\"Class distribution before under-sampling:\", Counter(y_train))\n",
    "\n",
    "# Under-sample\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(np.array(X_train).reshape(-1, 1), np.array(y_train))\n",
    "\n",
    "# Class distribution after under-sampling\n",
    "print(\"Class distribution after under-sampling:\", Counter(y_train_resampled))\n",
    "\n",
    "# Create pipeline with SGD Classifier\n",
    "model_sgd = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(2, 5)),\n",
    "    SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3, random_state=42)\n",
    ")\n",
    "\n",
    "# Train\n",
    "model_sgd.fit(X_train_resampled.flatten(), y_train_resampled)\n",
    "\n",
    "# Predict\n",
    "y_pred_sgd = model_sgd.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_sgd = accuracy_score(y_test, y_pred_sgd)\n",
    "print(f\"\\nSGD Classifier Accuracy: {accuracy_sgd * 100:.2f}%\")\n",
    "print(\"Classification Report (SGD Classifier):\")\n",
    "print(classification_report(y_test, y_pred_sgd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24b85123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [11:11:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 62.60%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.38      0.37      2130\n",
      "         1.0       0.47      0.60      0.53      1237\n",
      "         2.0       0.12      0.39      0.19       644\n",
      "         3.0       0.34      0.77      0.47      1222\n",
      "         4.0       0.75      0.74      0.75       994\n",
      "         5.0       0.49      0.54      0.52      2183\n",
      "         6.0       0.88      0.62      0.73      9523\n",
      "         7.0       0.43      0.71      0.54       665\n",
      "         8.0       0.92      0.71      0.80      5791\n",
      "         9.0       0.63      0.80      0.70       761\n",
      "\n",
      "    accuracy                           0.63     25150\n",
      "   macro avg       0.54      0.63      0.56     25150\n",
      "weighted avg       0.72      0.63      0.66     25150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ensemble(LR+Xgboost)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# Clean label column\n",
    "df['Label'] = pd.to_numeric(df['Label'], errors='coerce')\n",
    "df = df.dropna(subset=['Label'])\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Query'], df['Label'], test_size=0.25, random_state=42)\n",
    "\n",
    "# Under-sampling\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train.values.reshape(-1, 1), y_train)\n",
    "\n",
    "# Flatten input back to text\n",
    "X_train_resampled = X_train_resampled.flatten()\n",
    "\n",
    "# TF-IDF vectorizer (shared)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 5))\n",
    "\n",
    "# Transform training and test data\n",
    "X_train_vec = vectorizer.fit_transform(X_train_resampled)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Create classifiers\n",
    "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Voting Classifier\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('lr', lr), ('xgb', xgb)],\n",
    "    voting='soft'  # use 'soft' voting for probabilities\n",
    ")\n",
    "\n",
    "# Train\n",
    "ensemble.fit(X_train_vec, y_train_resampled)\n",
    "\n",
    "# Predict\n",
    "y_pred = ensemble.predict(X_test_vec)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Ensemble Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58aca732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”€ Ensemble 1: NB + LR + SVM\n",
      "âœ… Accuracy: 65.39%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.35      0.38      2130\n",
      "         1.0       0.44      0.61      0.51      1237\n",
      "         2.0       0.16      0.37      0.22       644\n",
      "         3.0       0.42      0.75      0.54      1222\n",
      "         4.0       0.69      0.78      0.73       994\n",
      "         5.0       0.49      0.58      0.53      2183\n",
      "         6.0       0.88      0.67      0.76      9523\n",
      "         7.0       0.42      0.75      0.54       665\n",
      "         8.0       0.91      0.73      0.81      5791\n",
      "         9.0       0.58      0.83      0.68       761\n",
      "\n",
      "    accuracy                           0.65     25150\n",
      "   macro avg       0.54      0.64      0.57     25150\n",
      "weighted avg       0.72      0.65      0.67     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”€ Ensemble 2: NB + LR + RF\n",
      "âœ… Accuracy: 64.93%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.34      0.37      2130\n",
      "         1.0       0.44      0.61      0.51      1237\n",
      "         2.0       0.12      0.46      0.19       644\n",
      "         3.0       0.52      0.69      0.60      1222\n",
      "         4.0       0.74      0.76      0.75       994\n",
      "         5.0       0.50      0.57      0.53      2183\n",
      "         6.0       0.88      0.66      0.76      9523\n",
      "         7.0       0.46      0.74      0.56       665\n",
      "         8.0       0.90      0.74      0.82      5791\n",
      "         9.0       0.56      0.83      0.67       761\n",
      "\n",
      "    accuracy                           0.65     25150\n",
      "   macro avg       0.55      0.64      0.58     25150\n",
      "weighted avg       0.73      0.65      0.68     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”€ Ensemble 3: NB + LR + SGD\n",
      "âœ… Accuracy: 65.70%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.34      0.38      2130\n",
      "         1.0       0.44      0.60      0.51      1237\n",
      "         2.0       0.15      0.32      0.21       644\n",
      "         3.0       0.41      0.76      0.53      1222\n",
      "         4.0       0.71      0.77      0.74       994\n",
      "         5.0       0.49      0.57      0.53      2183\n",
      "         6.0       0.87      0.68      0.76      9523\n",
      "         7.0       0.46      0.76      0.57       665\n",
      "         8.0       0.90      0.75      0.82      5791\n",
      "         9.0       0.55      0.83      0.66       761\n",
      "\n",
      "    accuracy                           0.66     25150\n",
      "   macro avg       0.54      0.64      0.57     25150\n",
      "weighted avg       0.72      0.66      0.67     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [11:39:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”€ Ensemble 4: NB + LR + XGB\n",
      "âœ… Accuracy: 65.65%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.34      0.37      2130\n",
      "         1.0       0.45      0.62      0.52      1237\n",
      "         2.0       0.15      0.39      0.22       644\n",
      "         3.0       0.41      0.76      0.53      1222\n",
      "         4.0       0.72      0.77      0.74       994\n",
      "         5.0       0.49      0.56      0.53      2183\n",
      "         6.0       0.88      0.67      0.76      9523\n",
      "         7.0       0.48      0.76      0.59       665\n",
      "         8.0       0.90      0.75      0.82      5791\n",
      "         9.0       0.56      0.83      0.67       761\n",
      "\n",
      "    accuracy                           0.66     25150\n",
      "   macro avg       0.55      0.64      0.58     25150\n",
      "weighted avg       0.72      0.66      0.68     25150\n",
      "\n",
      "\n",
      "ðŸ”€ Ensemble 5: NB + SVM + RF\n",
      "âœ… Accuracy: 64.61%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.34      0.38      2130\n",
      "         1.0       0.44      0.61      0.51      1237\n",
      "         2.0       0.12      0.45      0.19       644\n",
      "         3.0       0.54      0.69      0.60      1222\n",
      "         4.0       0.71      0.77      0.74       994\n",
      "         5.0       0.49      0.57      0.53      2183\n",
      "         6.0       0.88      0.66      0.76      9523\n",
      "         7.0       0.41      0.75      0.53       665\n",
      "         8.0       0.91      0.72      0.80      5791\n",
      "         9.0       0.56      0.83      0.67       761\n",
      "\n",
      "    accuracy                           0.65     25150\n",
      "   macro avg       0.55      0.64      0.57     25150\n",
      "weighted avg       0.73      0.65      0.67     25150\n",
      "\n",
      "\n",
      "ðŸ”€ Ensemble 6: NB + SVM + SGD\n",
      "âœ… Accuracy: 65.40%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.35      0.38      2130\n",
      "         1.0       0.44      0.61      0.51      1237\n",
      "         2.0       0.16      0.37      0.22       644\n",
      "         3.0       0.42      0.75      0.54      1222\n",
      "         4.0       0.69      0.78      0.73       994\n",
      "         5.0       0.48      0.58      0.53      2183\n",
      "         6.0       0.88      0.67      0.76      9523\n",
      "         7.0       0.42      0.76      0.54       665\n",
      "         8.0       0.91      0.73      0.81      5791\n",
      "         9.0       0.57      0.83      0.67       761\n",
      "\n",
      "    accuracy                           0.65     25150\n",
      "   macro avg       0.54      0.64      0.57     25150\n",
      "weighted avg       0.72      0.65      0.67     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [11:54:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”€ Ensemble 7: NB + SVM + XGB\n",
      "âœ… Accuracy: 65.43%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.35      0.38      2130\n",
      "         1.0       0.45      0.62      0.52      1237\n",
      "         2.0       0.15      0.38      0.22       644\n",
      "         3.0       0.42      0.75      0.54      1222\n",
      "         4.0       0.70      0.78      0.74       994\n",
      "         5.0       0.49      0.58      0.53      2183\n",
      "         6.0       0.88      0.67      0.76      9523\n",
      "         7.0       0.42      0.75      0.54       665\n",
      "         8.0       0.91      0.73      0.81      5791\n",
      "         9.0       0.57      0.83      0.67       761\n",
      "\n",
      "    accuracy                           0.65     25150\n",
      "   macro avg       0.54      0.64      0.57     25150\n",
      "weighted avg       0.72      0.65      0.67     25150\n",
      "\n",
      "\n",
      "ðŸ”€ Ensemble 8: NB + RF + SGD\n",
      "âœ… Accuracy: 64.80%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.33      0.37      2130\n",
      "         1.0       0.44      0.61      0.51      1237\n",
      "         2.0       0.12      0.46      0.19       644\n",
      "         3.0       0.52      0.69      0.59      1222\n",
      "         4.0       0.74      0.76      0.75       994\n",
      "         5.0       0.50      0.56      0.53      2183\n",
      "         6.0       0.88      0.66      0.75      9523\n",
      "         7.0       0.45      0.75      0.56       665\n",
      "         8.0       0.90      0.74      0.82      5791\n",
      "         9.0       0.55      0.83      0.66       761\n",
      "\n",
      "    accuracy                           0.65     25150\n",
      "   macro avg       0.55      0.64      0.57     25150\n",
      "weighted avg       0.73      0.65      0.67     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [12:21:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”€ Ensemble 9: NB + RF + XGB\n",
      "âœ… Accuracy: 64.05%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.34      0.37      2130\n",
      "         1.0       0.44      0.62      0.52      1237\n",
      "         2.0       0.11      0.48      0.18       644\n",
      "         3.0       0.50      0.71      0.59      1222\n",
      "         4.0       0.76      0.75      0.76       994\n",
      "         5.0       0.52      0.53      0.52      2183\n",
      "         6.0       0.88      0.65      0.75      9523\n",
      "         7.0       0.46      0.74      0.56       665\n",
      "         8.0       0.91      0.74      0.81      5791\n",
      "         9.0       0.54      0.83      0.66       761\n",
      "\n",
      "    accuracy                           0.64     25150\n",
      "   macro avg       0.55      0.64      0.57     25150\n",
      "weighted avg       0.73      0.64      0.67     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [12:21:43] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”€ Ensemble 10: NB + SGD + XGB\n",
      "âœ… Accuracy: 65.66%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.34      0.37      2130\n",
      "         1.0       0.45      0.62      0.52      1237\n",
      "         2.0       0.15      0.38      0.22       644\n",
      "         3.0       0.41      0.77      0.54      1222\n",
      "         4.0       0.71      0.77      0.74       994\n",
      "         5.0       0.49      0.56      0.52      2183\n",
      "         6.0       0.88      0.67      0.76      9523\n",
      "         7.0       0.47      0.76      0.58       665\n",
      "         8.0       0.90      0.75      0.82      5791\n",
      "         9.0       0.56      0.83      0.67       761\n",
      "\n",
      "    accuracy                           0.66     25150\n",
      "   macro avg       0.54      0.64      0.57     25150\n",
      "weighted avg       0.72      0.66      0.68     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”€ Ensemble 11: LR + SVM + RF\n",
      "âœ… Accuracy: 63.86%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.35      0.37      2130\n",
      "         1.0       0.44      0.61      0.51      1237\n",
      "         2.0       0.11      0.47      0.18       644\n",
      "         3.0       0.50      0.67      0.57      1222\n",
      "         4.0       0.72      0.77      0.74       994\n",
      "         5.0       0.49      0.57      0.53      2183\n",
      "         6.0       0.89      0.65      0.75      9523\n",
      "         7.0       0.41      0.73      0.53       665\n",
      "         8.0       0.91      0.71      0.80      5791\n",
      "         9.0       0.61      0.82      0.70       761\n",
      "\n",
      "    accuracy                           0.64     25150\n",
      "   macro avg       0.55      0.63      0.57     25150\n",
      "weighted avg       0.73      0.64      0.67     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”€ Ensemble 12: LR + SVM + SGD\n",
      "âœ… Accuracy: 65.16%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.36      0.38      2130\n",
      "         1.0       0.46      0.61      0.52      1237\n",
      "         2.0       0.15      0.36      0.21       644\n",
      "         3.0       0.39      0.75      0.52      1222\n",
      "         4.0       0.71      0.77      0.74       994\n",
      "         5.0       0.48      0.58      0.53      2183\n",
      "         6.0       0.88      0.67      0.76      9523\n",
      "         7.0       0.43      0.75      0.55       665\n",
      "         8.0       0.91      0.72      0.81      5791\n",
      "         9.0       0.62      0.81      0.70       761\n",
      "\n",
      "    accuracy                           0.65     25150\n",
      "   macro avg       0.54      0.64      0.57     25150\n",
      "weighted avg       0.72      0.65      0.67     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [17:16:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”€ Ensemble 13: LR + SVM + XGB\n",
      "âœ… Accuracy: 65.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.38      0.39      2130\n",
      "         1.0       0.46      0.62      0.53      1237\n",
      "         2.0       0.15      0.39      0.22       644\n",
      "         3.0       0.39      0.75      0.51      1222\n",
      "         4.0       0.71      0.77      0.74       994\n",
      "         5.0       0.49      0.58      0.53      2183\n",
      "         6.0       0.88      0.66      0.75      9523\n",
      "         7.0       0.44      0.74      0.55       665\n",
      "         8.0       0.91      0.73      0.81      5791\n",
      "         9.0       0.62      0.81      0.70       761\n",
      "\n",
      "    accuracy                           0.65     25150\n",
      "   macro avg       0.55      0.64      0.57     25150\n",
      "weighted avg       0.73      0.65      0.67     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”€ Ensemble 14: LR + RF + SGD\n",
      "âœ… Accuracy: 63.12%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.34      0.36      2130\n",
      "         1.0       0.45      0.60      0.51      1237\n",
      "         2.0       0.11      0.48      0.17       644\n",
      "         3.0       0.47      0.67      0.55      1222\n",
      "         4.0       0.75      0.74      0.74       994\n",
      "         5.0       0.50      0.55      0.53      2183\n",
      "         6.0       0.89      0.63      0.74      9523\n",
      "         7.0       0.44      0.71      0.54       665\n",
      "         8.0       0.90      0.73      0.81      5791\n",
      "         9.0       0.61      0.81      0.69       761\n",
      "\n",
      "    accuracy                           0.63     25150\n",
      "   macro avg       0.55      0.63      0.57     25150\n",
      "weighted avg       0.73      0.63      0.66     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [17:30:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”€ Ensemble 15: LR + RF + XGB\n",
      "âœ… Accuracy: 60.93%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.35      0.36      2130\n",
      "         1.0       0.45      0.60      0.52      1237\n",
      "         2.0       0.10      0.50      0.16       644\n",
      "         3.0       0.46      0.66      0.54      1222\n",
      "         4.0       0.74      0.74      0.74       994\n",
      "         5.0       0.50      0.53      0.52      2183\n",
      "         6.0       0.89      0.61      0.72      9523\n",
      "         7.0       0.38      0.71      0.49       665\n",
      "         8.0       0.91      0.69      0.78      5791\n",
      "         9.0       0.60      0.81      0.69       761\n",
      "\n",
      "    accuracy                           0.61     25150\n",
      "   macro avg       0.54      0.62      0.55     25150\n",
      "weighted avg       0.73      0.61      0.65     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [17:31:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”€ Ensemble 16: LR + SGD + XGB\n",
      "âœ… Accuracy: 63.96%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.37      0.38      0.37      2130\n",
      "         1.0       0.48      0.60      0.54      1237\n",
      "         2.0       0.14      0.38      0.20       644\n",
      "         3.0       0.34      0.77      0.47      1222\n",
      "         4.0       0.76      0.74      0.75       994\n",
      "         5.0       0.49      0.56      0.52      2183\n",
      "         6.0       0.88      0.64      0.74      9523\n",
      "         7.0       0.48      0.71      0.58       665\n",
      "         8.0       0.91      0.74      0.81      5791\n",
      "         9.0       0.64      0.81      0.71       761\n",
      "\n",
      "    accuracy                           0.64     25150\n",
      "   macro avg       0.55      0.63      0.57     25150\n",
      "weighted avg       0.72      0.64      0.67     25150\n",
      "\n",
      "\n",
      "ðŸ”€ Ensemble 17: SVM + RF + SGD\n",
      "âœ… Accuracy: 63.76%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.34      0.37      2130\n",
      "         1.0       0.44      0.61      0.51      1237\n",
      "         2.0       0.11      0.47      0.18       644\n",
      "         3.0       0.51      0.67      0.58      1222\n",
      "         4.0       0.71      0.77      0.74       994\n",
      "         5.0       0.49      0.57      0.53      2183\n",
      "         6.0       0.89      0.65      0.75      9523\n",
      "         7.0       0.41      0.73      0.53       665\n",
      "         8.0       0.91      0.71      0.80      5791\n",
      "         9.0       0.59      0.82      0.69       761\n",
      "\n",
      "    accuracy                           0.64     25150\n",
      "   macro avg       0.55      0.63      0.57     25150\n",
      "weighted avg       0.73      0.64      0.67     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [17:51:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”€ Ensemble 18: SVM + RF + XGB\n",
      "âœ… Accuracy: 62.91%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.35      0.37      2130\n",
      "         1.0       0.44      0.61      0.51      1237\n",
      "         2.0       0.11      0.48      0.18       644\n",
      "         3.0       0.49      0.67      0.57      1222\n",
      "         4.0       0.72      0.76      0.74       994\n",
      "         5.0       0.49      0.55      0.52      2183\n",
      "         6.0       0.89      0.63      0.74      9523\n",
      "         7.0       0.41      0.73      0.53       665\n",
      "         8.0       0.92      0.71      0.80      5791\n",
      "         9.0       0.60      0.81      0.69       761\n",
      "\n",
      "    accuracy                           0.63     25150\n",
      "   macro avg       0.55      0.63      0.56     25150\n",
      "weighted avg       0.73      0.63      0.66     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [17:54:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”€ Ensemble 19: SVM + SGD + XGB\n",
      "âœ… Accuracy: 64.96%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.38      0.39      2130\n",
      "         1.0       0.47      0.62      0.53      1237\n",
      "         2.0       0.15      0.39      0.22       644\n",
      "         3.0       0.39      0.75      0.51      1222\n",
      "         4.0       0.71      0.77      0.74       994\n",
      "         5.0       0.49      0.58      0.53      2183\n",
      "         6.0       0.88      0.66      0.75      9523\n",
      "         7.0       0.44      0.74      0.55       665\n",
      "         8.0       0.91      0.73      0.81      5791\n",
      "         9.0       0.61      0.81      0.70       761\n",
      "\n",
      "    accuracy                           0.65     25150\n",
      "   macro avg       0.55      0.64      0.57     25150\n",
      "weighted avg       0.73      0.65      0.67     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [18:01:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”€ Ensemble 20: RF + SGD + XGB\n",
      "âœ… Accuracy: 59.93%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.35      0.35      2130\n",
      "         1.0       0.44      0.59      0.50      1237\n",
      "         2.0       0.09      0.49      0.16       644\n",
      "         3.0       0.44      0.65      0.52      1222\n",
      "         4.0       0.74      0.74      0.74       994\n",
      "         5.0       0.51      0.52      0.51      2183\n",
      "         6.0       0.89      0.59      0.71      9523\n",
      "         7.0       0.39      0.70      0.50       665\n",
      "         8.0       0.91      0.68      0.78      5791\n",
      "         9.0       0.58      0.81      0.68       761\n",
      "\n",
      "    accuracy                           0.60     25150\n",
      "   macro avg       0.53      0.61      0.54     25150\n",
      "weighted avg       0.72      0.60      0.64     25150\n",
      "\n",
      "\n",
      "ðŸ“Š All Ensemble Results (Top to Bottom):\n",
      "           Ensemble  Accuracy\n",
      "0     NB + LR + SGD  0.656978\n",
      "1    NB + SGD + XGB  0.656620\n",
      "2     NB + LR + XGB  0.656501\n",
      "3    NB + SVM + XGB  0.654314\n",
      "4    NB + SVM + SGD  0.654036\n",
      "5     NB + LR + SVM  0.653917\n",
      "6    LR + SVM + SGD  0.651610\n",
      "7    LR + SVM + XGB  0.650020\n",
      "8   SVM + SGD + XGB  0.649622\n",
      "9      NB + LR + RF  0.649344\n",
      "10    NB + RF + SGD  0.647992\n",
      "11    NB + SVM + RF  0.646083\n",
      "12    NB + RF + XGB  0.640517\n",
      "13   LR + SGD + XGB  0.639563\n",
      "14    LR + SVM + RF  0.638608\n",
      "15   SVM + RF + SGD  0.637575\n",
      "16    LR + RF + SGD  0.631173\n",
      "17   SVM + RF + XGB  0.629145\n",
      "18    LR + RF + XGB  0.609344\n",
      "19   RF + SGD + XGB  0.599284\n"
     ]
    }
   ],
   "source": [
    "#essemble for every 3 models and give 20 combinations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Clean and prepare your data\n",
    "df['Label'] = pd.to_numeric(df['Label'], errors='coerce')\n",
    "df = df.dropna(subset=['Label'])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Query'], df['Label'], test_size=0.25, random_state=42)\n",
    "\n",
    "# Under-sampling\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(np.array(X_train).reshape(-1, 1), y_train)\n",
    "X_train_resampled = X_train_resampled.flatten()\n",
    "\n",
    "# Define models with names and actual estimators\n",
    "models_dict = {\n",
    "    'NB': MultinomialNB(alpha=0.1),\n",
    "    'LR': LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000),\n",
    "    'SVM': SVC(probability=True, kernel='linear'),\n",
    "    'RF': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SGD': SGDClassifier(loss='log_loss', max_iter=1000, random_state=42),\n",
    "    'XGB': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "}\n",
    "\n",
    "# Generate all 20 unique 3-model combinations\n",
    "model_combos = list(combinations(models_dict.keys(), 3))\n",
    "results = []\n",
    "\n",
    "# Train and evaluate each ensemble\n",
    "for i, combo in enumerate(model_combos, start=1):\n",
    "    estimators = [(name, models_dict[name]) for name in combo]\n",
    "    ensemble = VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "    # Create pipeline with TF-IDF and ensemble model\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(ngram_range=(2, 5)),\n",
    "        ensemble\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nðŸ”€ Ensemble {i}: {' + '.join(combo)}\")\n",
    "    print(f\"âœ… Accuracy: {acc * 100:.2f}%\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    results.append({\n",
    "        'Ensemble': ' + '.join(combo),\n",
    "        'Accuracy': acc\n",
    "    })\n",
    "\n",
    "# Show all results sorted by accuracy\n",
    "results_df = pd.DataFrame(results).sort_values(by='Accuracy', ascending=False)\n",
    "print(\"\\nðŸ“Š All Ensemble Results (Top to Bottom):\")\n",
    "print(results_df.reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "260be1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒŸ 5-Model Ensemble 1: NB + LR + SVM + RF + SGD\n",
      "âœ… Accuracy: 64.92%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.34      0.38      2130\n",
      "         1.0       0.45      0.61      0.52      1237\n",
      "         2.0       0.12      0.44      0.19       644\n",
      "         3.0       0.53      0.69      0.60      1222\n",
      "         4.0       0.71      0.77      0.74       994\n",
      "         5.0       0.50      0.58      0.54      2183\n",
      "         6.0       0.88      0.67      0.76      9523\n",
      "         7.0       0.42      0.75      0.54       665\n",
      "         8.0       0.91      0.72      0.81      5791\n",
      "         9.0       0.59      0.83      0.69       761\n",
      "\n",
      "    accuracy                           0.65     25150\n",
      "   macro avg       0.55      0.64      0.57     25150\n",
      "weighted avg       0.73      0.65      0.68     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [18:22:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒŸ 5-Model Ensemble 2: NB + LR + SVM + RF + XGB\n",
      "âœ… Accuracy: 64.64%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.35      0.38      2130\n",
      "         1.0       0.45      0.62      0.52      1237\n",
      "         2.0       0.12      0.46      0.19       644\n",
      "         3.0       0.52      0.69      0.60      1222\n",
      "         4.0       0.72      0.77      0.74       994\n",
      "         5.0       0.50      0.57      0.53      2183\n",
      "         6.0       0.88      0.66      0.76      9523\n",
      "         7.0       0.42      0.74      0.54       665\n",
      "         8.0       0.91      0.72      0.81      5791\n",
      "         9.0       0.58      0.83      0.68       761\n",
      "\n",
      "    accuracy                           0.65     25150\n",
      "   macro avg       0.55      0.64      0.57     25150\n",
      "weighted avg       0.73      0.65      0.67     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [18:25:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒŸ 5-Model Ensemble 3: NB + LR + SVM + SGD + XGB\n",
      "âœ… Accuracy: 65.68%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.36      0.39      2130\n",
      "         1.0       0.46      0.62      0.52      1237\n",
      "         2.0       0.16      0.38      0.22       644\n",
      "         3.0       0.41      0.76      0.53      1222\n",
      "         4.0       0.70      0.78      0.74       994\n",
      "         5.0       0.49      0.58      0.53      2183\n",
      "         6.0       0.88      0.67      0.76      9523\n",
      "         7.0       0.43      0.76      0.55       665\n",
      "         8.0       0.91      0.73      0.81      5791\n",
      "         9.0       0.59      0.83      0.69       761\n",
      "\n",
      "    accuracy                           0.66     25150\n",
      "   macro avg       0.55      0.65      0.58     25150\n",
      "weighted avg       0.72      0.66      0.68     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [18:46:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒŸ 5-Model Ensemble 4: NB + LR + RF + SGD + XGB\n",
      "âœ… Accuracy: 64.91%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.34      0.37      2130\n",
      "         1.0       0.45      0.62      0.52      1237\n",
      "         2.0       0.12      0.47      0.19       644\n",
      "         3.0       0.51      0.70      0.59      1222\n",
      "         4.0       0.75      0.76      0.75       994\n",
      "         5.0       0.51      0.56      0.53      2183\n",
      "         6.0       0.88      0.66      0.75      9523\n",
      "         7.0       0.46      0.73      0.57       665\n",
      "         8.0       0.91      0.74      0.82      5791\n",
      "         9.0       0.58      0.83      0.68       761\n",
      "\n",
      "    accuracy                           0.65     25150\n",
      "   macro avg       0.56      0.64      0.58     25150\n",
      "weighted avg       0.73      0.65      0.68     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [18:55:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒŸ 5-Model Ensemble 5: NB + SVM + RF + SGD + XGB\n",
      "âœ… Accuracy: 64.70%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.34      0.37      2130\n",
      "         1.0       0.45      0.62      0.52      1237\n",
      "         2.0       0.12      0.46      0.19       644\n",
      "         3.0       0.52      0.69      0.60      1222\n",
      "         4.0       0.72      0.77      0.74       994\n",
      "         5.0       0.50      0.57      0.53      2183\n",
      "         6.0       0.88      0.66      0.76      9523\n",
      "         7.0       0.42      0.74      0.54       665\n",
      "         8.0       0.91      0.73      0.81      5791\n",
      "         9.0       0.58      0.83      0.68       761\n",
      "\n",
      "    accuracy                           0.65     25150\n",
      "   macro avg       0.55      0.64      0.57     25150\n",
      "weighted avg       0.73      0.65      0.67     25150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/usr/local/Caskroom/miniforge/base/envs/new_env/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [19:05:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒŸ 5-Model Ensemble 6: LR + SVM + RF + SGD + XGB\n",
      "âœ… Accuracy: 64.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.35      0.38      2130\n",
      "         1.0       0.45      0.61      0.52      1237\n",
      "         2.0       0.11      0.47      0.18       644\n",
      "         3.0       0.50      0.67      0.57      1222\n",
      "         4.0       0.72      0.76      0.74       994\n",
      "         5.0       0.50      0.57      0.53      2183\n",
      "         6.0       0.89      0.65      0.75      9523\n",
      "         7.0       0.43      0.73      0.54       665\n",
      "         8.0       0.91      0.72      0.81      5791\n",
      "         9.0       0.62      0.81      0.70       761\n",
      "\n",
      "    accuracy                           0.64     25150\n",
      "   macro avg       0.55      0.64      0.57     25150\n",
      "weighted avg       0.73      0.64      0.67     25150\n",
      "\n",
      "\n",
      "ðŸ“Š All 5-Model Ensemble Results:\n",
      "                    Ensemble  Accuracy\n",
      "0  NB + LR + SVM + SGD + XGB  0.656819\n",
      "1   NB + LR + SVM + RF + SGD  0.649225\n",
      "2   NB + LR + RF + SGD + XGB  0.649066\n",
      "3  NB + SVM + RF + SGD + XGB  0.647038\n",
      "4   NB + LR + SVM + RF + XGB  0.646441\n",
      "5  LR + SVM + RF + SGD + XGB  0.639960\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Clean and prepare your data\n",
    "df['Label'] = pd.to_numeric(df['Label'], errors='coerce')\n",
    "df = df.dropna(subset=['Label'])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Query'], df['Label'], test_size=0.25, random_state=42)\n",
    "\n",
    "# Under-sampling\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(np.array(X_train).reshape(-1, 1), y_train)\n",
    "X_train_resampled = X_train_resampled.flatten()\n",
    "\n",
    "# Define the 6 models\n",
    "models_dict = {\n",
    "    'NB': MultinomialNB(alpha=0.1),\n",
    "    'LR': LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000),\n",
    "    'SVM': SVC(probability=True, kernel='linear'),\n",
    "    'RF': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SGD': SGDClassifier(loss='log_loss', max_iter=1000, random_state=42),\n",
    "    'XGB': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "}\n",
    "\n",
    "# Get all 5-model combinations\n",
    "five_model_combos = list(combinations(models_dict.keys(), 5))\n",
    "results = []\n",
    "\n",
    "# Evaluate each 5-model ensemble\n",
    "for i, combo in enumerate(five_model_combos, start=1):\n",
    "    estimators = [(name, models_dict[name]) for name in combo]\n",
    "    ensemble = VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(ngram_range=(2, 5)),\n",
    "        ensemble\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nðŸŒŸ 5-Model Ensemble {i}: {' + '.join(combo)}\")\n",
    "    print(f\"âœ… Accuracy: {acc * 100:.2f}%\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    results.append({\n",
    "        'Ensemble': ' + '.join(combo),\n",
    "        'Accuracy': acc\n",
    "    })\n",
    "\n",
    "# Results table\n",
    "results_df = pd.DataFrame(results).sort_values(by='Accuracy', ascending=False)\n",
    "print(\"\\nðŸ“Š All 5-Model Ensemble Results:\")\n",
    "print(results_df.reset_index(drop=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e8421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
